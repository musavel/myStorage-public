from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from typing import List, Dict, Any, Optional, Literal
import json
import logging

from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.prebuilt import create_react_agent

from backend.app.core.config import settings
from backend.app.core.auth import require_owner
from backend.app.core.ai_model_manager import get_model_manager

router = APIRouter(prefix="/ai", tags=["ai"])
logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜ë¡œ í˜„ì¬ ì„¤ì • ì €ì¥ (DB ëŒ€ì‹  ë©”ëª¨ë¦¬ ì‚¬ìš©)
current_ai_settings = {
    "text_model": None,
    "vision_model": None
}


class FieldSuggestion(BaseModel):
    """AIê°€ ì¶”ì²œí•œ í•„ë“œ"""
    key: str
    label: str
    type: Literal["text", "textarea", "number", "date", "select"]
    required: bool = False
    placeholder: Optional[str] = None
    options: Optional[List[str]] = None
    help_text: Optional[str] = None


class FieldSuggestionResponse(BaseModel):
    """í•„ë“œ ì¶”ì²œ ì‘ë‹µ"""
    fields: List[FieldSuggestion]
    provider: str  # "openai" or "gemini"


class SuggestFieldsRequest(BaseModel):
    """í•„ë“œ ì¶”ì²œ ìš”ì²­"""
    collection_name: str
    description: Optional[str] = None
    provider: Optional[Literal["openai", "gemini"]] = None  # Noneì´ë©´ ì„¤ì •ëœ ëª¨ë¸ ì‚¬ìš©
    model_id: Optional[str] = None  # íŠ¹ì • ëª¨ë¸ ì§€ì •


SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì»¬ë ‰ì…˜ ê´€ë¦¬ ì‹œìŠ¤í…œì˜ ë©”íƒ€ë°ì´í„° í•„ë“œ ì„¤ê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìê°€ ì»¬ë ‰ì…˜ ì´ë¦„ì„ ì œê³µí•˜ë©´, í•´ë‹¹ ì»¬ë ‰ì…˜ì— ì í•©í•œ ë©”íƒ€ë°ì´í„° í•„ë“œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

**ì¤‘ìš” ê·œì¹™:**
1. keyëŠ” ì˜ë¬¸ ì†Œë¬¸ìì™€ ì–¸ë”ìŠ¤ì½”ì–´ë§Œ ì‚¬ìš© (snake_case)
2. labelì€ í•œêµ­ì–´ë¡œ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì§ˆ ì´ë¦„
3. typeì€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜: text, textarea, number, date, select
4. select íƒ€ì…ì¼ ê²½ìš° options ë°°ì—´ í•„ìˆ˜
5. placeholderëŠ” ì…ë ¥ ì˜ˆì‹œë¥¼ ì œê³µ
6. ì¼ë°˜ì ìœ¼ë¡œ 5-15ê°œ í•„ë“œ ì¶”ì²œ
7. ê¸°ë³¸ í•„ë“œ: description(ì„¤ëª…), image_url(ì´ë¯¸ì§€), purchase_date(êµ¬ë§¤ì¼), purchase_price(êµ¬ë§¤ê°€ê²©), location(ë³´ê´€ìœ„ì¹˜), notes(ë©”ëª¨) í¬í•¨

**í•„ë“œ íƒ€ì… ê°€ì´ë“œ:**
- text: ì§§ì€ í…ìŠ¤íŠ¸ (ì´ë¦„, ì œëª©, ISBN ë“±)
- textarea: ê¸´ í…ìŠ¤íŠ¸ (ì„¤ëª…, ë©”ëª¨ ë“±)
- number: ìˆ«ì (ê°€ê²©, í˜ì´ì§€ ìˆ˜, ì¸ì› ë“±)
- date: ë‚ ì§œ (êµ¬ë§¤ì¼, ì¶œíŒì¼ ë“±)
- select: ì„ íƒ ì˜µì…˜ (ì¹´í…Œê³ ë¦¬, ì¥ë¥´, ìƒíƒœ ë“±)

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{
  "fields": [
    {
      "key": "artist",
      "label": "ì•„í‹°ìŠ¤íŠ¸",
      "type": "text",
      "required": false,
      "placeholder": "ì˜ˆ: The Beatles"
    }
  ]
}
"""


def create_llm(provider: str, model_id: str = None):
    """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    if provider == "openai":
        if not settings.OPENAI_API_KEY:
            raise HTTPException(status_code=400, detail="OpenAI API key not configured")

        # ëª¨ë¸ IDê°€ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not model_id:
            model_id = "gpt-4o-mini"

        return ChatOpenAI(
            model=model_id,
            api_key=settings.OPENAI_API_KEY,
            temperature=0.7,
        )
    elif provider == "gemini":
        if not settings.GEMINI_API_KEY:
            raise HTTPException(status_code=400, detail="Gemini API key not configured")

        # Gemini ëª¨ë¸ ID ë§¤í•‘ (langchain-google-genai í˜•ì‹)
        gemini_model_mapping = {
            "gemini-2.5-flash": "gemini-2.0-flash-exp",
            "gemini-2.5-flash-lite": "gemini-2.0-flash-exp",
            "gemini-2.5-pro": "gemini-2.0-flash-exp",
        }

        if not model_id:
            model_id = "gemini-2.0-flash-exp"
        elif model_id in gemini_model_mapping:
            model_id = gemini_model_mapping[model_id]

        return ChatGoogleGenerativeAI(
            model=model_id,
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.7,
        )
    else:
        raise HTTPException(status_code=400, detail="Unsupported provider")


@router.post("/suggest-fields", response_model=FieldSuggestionResponse)
async def suggest_fields(
    request: SuggestFieldsRequest,
    email: str = Depends(require_owner)
):
    """AI ê¸°ë°˜ ì»¬ë ‰ì…˜ í•„ë“œ ì¶”ì²œ (Owner only)"""

    try:
        # providerì™€ model_id ê²°ì •
        provider = request.provider
        model_id = request.model_id

        # providerê°€ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ì„¤ì •ëœ í…ìŠ¤íŠ¸ ëª¨ë¸ ì‚¬ìš©
        if not provider:
            if current_ai_settings["text_model"]:
                provider = current_ai_settings["text_model"]["provider"]
                model_id = current_ai_settings["text_model"]["model_id"]
                logger.info(f"ğŸ“ ì„¤ì •ëœ ëª¨ë¸ ì‚¬ìš©: {provider}/{model_id}")
            else:
                # ê¸°ë³¸ê°’: OpenAI
                provider = "openai"
                model_id = "gpt-4o-mini"
                logger.info(f"âš ï¸ ì„¤ì •ëœ ëª¨ë¸ ì—†ìŒ. ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {provider}/{model_id}")

        # LLM ìƒì„±
        llm = create_llm(provider, model_id)

        # ì‚¬ìš©ì ë©”ì‹œì§€ êµ¬ì„±
        user_message = f"ì»¬ë ‰ì…˜ ì´ë¦„: {request.collection_name}"
        if request.description:
            user_message += f"\nì„¤ëª…: {request.description}"

        # LangGraph Agent ìƒì„± (ë„êµ¬ ì—†ì´ ê°„ë‹¨í•œ ëŒ€í™”)
        messages = [
            SystemMessage(content=SYSTEM_PROMPT),
            HumanMessage(content=user_message)
        ]

        # LLM í˜¸ì¶œ
        response = await llm.ainvoke(messages)

        # JSON íŒŒì‹±
        try:
            # ì½”ë“œ ë¸”ë¡ ì œê±°
            content = response.content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()

            result = json.loads(content)

            if "fields" not in result:
                raise ValueError("Invalid response format: missing 'fields' key")

            return FieldSuggestionResponse(
                fields=[FieldSuggestion(**field) for field in result["fields"]],
                provider=request.provider
            )
        except json.JSONDecodeError as e:
            raise HTTPException(
                status_code=500,
                detail=f"Failed to parse AI response: {str(e)}\nResponse: {response.content[:200]}"
            )
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f"Failed to process AI response: {str(e)}"
            )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"AI request failed: {str(e)}"
        )


@router.get("/providers")
async def get_available_providers():
    """ì‚¬ìš© ê°€ëŠ¥í•œ AI ì œê³µì ëª©ë¡"""
    providers = []

    if settings.OPENAI_API_KEY:
        providers.append({
            "id": "openai",
            "name": "OpenAI (GPT-4o-mini)",
            "available": True
        })
    else:
        providers.append({
            "id": "openai",
            "name": "OpenAI (GPT-4o-mini)",
            "available": False,
            "reason": "API key not configured"
        })

    if settings.GEMINI_API_KEY:
        providers.append({
            "id": "gemini",
            "name": "Google Gemini 2.0 Flash",
            "available": True
        })
    else:
        providers.append({
            "id": "gemini",
            "name": "Google Gemini 2.0 Flash",
            "available": False,
            "reason": "API key not configured"
        })

    return {"providers": providers}


# ===== ëª¨ë¸ ì„ íƒ ê´€ë ¨ API =====

class ModelSelection(BaseModel):
    provider: str
    modelId: str


class AISettingsRequest(BaseModel):
    textModel: Optional[ModelSelection] = None
    visionModel: Optional[ModelSelection] = None


@router.get("/models")
async def get_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (ai_models.jsonì—ì„œ ë¡œë“œ)"""
    try:
        model_manager = get_model_manager()
        all_models = model_manager.get_all_models()

        # Pydantic ëª¨ë¸ì— ë§ê²Œ ë³€í™˜
        formatted_models = {}
        for provider, models in all_models.items():
            formatted_models[provider] = {}
            for model_id, config in models.items():
                formatted_models[provider][model_id] = {
                    "name": config.name,
                    "input_modalities": config.input_modalities,
                    "output_modalities": config.output_modalities,
                    "pricing": {
                        "input": config.pricing.input,
                        "output": config.pricing.output,
                        **({"input_audio": config.pricing.input_audio} if config.pricing.input_audio else {}),
                        **({"input_over_128k": config.pricing.input_over_128k} if config.pricing.input_over_128k else {}),
                        **({"output_over_128k": config.pricing.output_over_128k} if config.pricing.output_over_128k else {}),
                        **({"input_over_200k": config.pricing.input_over_200k} if config.pricing.input_over_200k else {}),
                        **({"output_over_200k": config.pricing.output_over_200k} if config.pricing.output_over_200k else {}),
                    }
                }

        return {
            "success": True,
            "models": formatted_models
        }

    except Exception as e:
        logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post("/set-models")
async def set_ai_models(
    settings_req: AISettingsRequest,
    email: str = Depends(require_owner)
):
    """í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì„ íƒí•œ AI ëª¨ë¸ ì„¤ì • ì €ì¥ (Owner only)"""
    global current_ai_settings

    try:
        # ì„¤ì • ì €ì¥
        if settings_req.textModel:
            current_ai_settings["text_model"] = {
                "provider": settings_req.textModel.provider,
                "model_id": settings_req.textModel.modelId
            }
            logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ëª¨ë¸ ì„¤ì •: {settings_req.textModel.provider}/{settings_req.textModel.modelId}")

        if settings_req.visionModel:
            current_ai_settings["vision_model"] = {
                "provider": settings_req.visionModel.provider,
                "model_id": settings_req.visionModel.modelId
            }
            logger.info(f"ğŸ‘ï¸ ë¹„ì „ ëª¨ë¸ ì„¤ì •: {settings_req.visionModel.provider}/{settings_req.visionModel.modelId}")

        logger.info(f"âœ… AI ëª¨ë¸ ì„¤ì • ì™„ë£Œ: {current_ai_settings}")

        return {
            "success": True,
            "message": "AI ëª¨ë¸ ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "settings": current_ai_settings
        }

    except Exception as e:
        logger.error(f"AI ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"AI ëª¨ë¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get("/get-models")
async def get_current_ai_models():
    """í˜„ì¬ ì„¤ì •ëœ AI ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
    return {
        "success": True,
        "settings": current_ai_settings
    }
