"""
ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
êµë³´ë¬¸ê³ ì™€ ì•Œë¼ë”˜ì—ì„œ ì‹¤ì œë¡œ ì¶”ì¶œ ê°€ëŠ¥í•œ ì •ë³´ë¥¼ í™•ì¸
"""
import asyncio
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from backend.app.services.scraper.web_scraper import WebScraper


async def main():
    urls = [
        ("êµë³´ë¬¸ê³ ", "https://product.kyobobook.co.kr/detail/S000001713046"),
        ("ì•Œë¼ë”˜", "https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=281358410"),
    ]

    async with WebScraper() as scraper:
        for site_name, url in urls:
            print(f"\n{'='*80}")
            print(f"ğŸ” {site_name}: {url}")
            print('='*80)

            try:
                result = await scraper.scrape_url(url)

                print("\nğŸ“‹ ì¶”ì¶œëœ ì •ë³´:")
                for key, value in sorted(result.items()):
                    if key == 'description' and value and len(str(value)) > 200:
                        print(f"  {key}: {str(value)[:200]}...")
                    else:
                        print(f"  {key}: {value}")

            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
